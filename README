A brief description of the solution:

The file task_data.csv contains an example data set that has been artificially generated.
The set consists of 400 samples where for each sample there are 10 different sensor readings available.
The samples have been divided into two classes where the class label is either 1 or -1.
The class labels define to what particular class a particular sample belongs.

My task:

I have to rank the sensors according to their importance/predictive power with respect to the class
labels of the samples. Your solution should be a Python script that generates a ranking of the sensors
from the provided CSV file.
The ranking should be in decreasing order where the first sensor is the most important one.

What I done:

* The use of Pandas library to parse all features, names of features and labels.
* The use of Scikit-Learn library to perform evaluation of Recursive Feature Elimination
  to predicting the target attribute. It uses the model accuracy to identify which attributes
  contribute the most to predicting the target attribute.
* The use of Scikit-Learn library to perform evaluation of feature importance.
  In this case I used two method of decision tree such as Extra Trees and Random Forest.
  Both methods are about the same, with the Extra Trees being a bit worse in high dimensional data-sets
  when there is a high number of noisy features. Manual provided the feature selection is near optimal,
  the performance is about the same, however, Extra Trees can be computationally faster.
  Extra Trees are generally cheaper to train from a computational point of view but can grow much bigger.
  Both conclusions are correct, although the Random Forest implementation in Scikit-learn makes it possible
  to enable or disable the bootstrap resampling. In practice, Random Forests are often more compact than Extra Trees.
  Extra Trees can sometime generalize better than Random Forests but it's hard to guess when it's the case
  without trying both first.
* Save as sorted in a CSV file.
* View all results in visual form and compare them.

*Summary:
  Feature selection methods can give you useful information on the relative importance or relevance of features 
  for a given problem. 
  You can use this information to create filtered versions of your dataset and increase the accuracy of your models.
